# MAHENet
Image material perception aims to achieve pixel-level accurate identification of the surface materials of objects in an image, and is a key step in building fine-grained visual understanding capabilities. To improve the accuracy and boundary quality of material perception in complex real-world scenes, this study proposes a novel material perception network, MAHENet. This network first extracts complementary global semantic context and multi-scale local texture features in parallel through a semantic-texture hybrid encoder based on Swin-T and dilated convolutional pyramids. To further enhance texture discriminative power, we designed a material perception attention module that incorporates Gabor filter priors to adaptively calibrate key feature channels. Subsequently, a multi-scale fusion module integrating feature pyramids is used to effectively integrate deep semantics and shallow details. Finally, an edge enhancement decoder, guided by auxiliary supervision signals, recovers a segmentation map with clear boundaries and rich details. The proposed method was tested multiple times on the publicly available DMS dataset. Experimental results show that MAHENet outperforms existing methods and has strong generalization ability, especially when processing materials with directional textures such as wood and fabric.
![image](https://github.com/wuhao0109/MAHENet/blob/main/images/Framework%20Diagram.jpg)
## Code
### Configure environment
```
conda env create -f environment.yaml
conda activate MAHENet
```
### Download pre-trained model
[download here](https://drive.google.com/file/d/1KbW3mG2Pz9ieXKotFQJ52YNNG8eBCNko/view?usp=sharing)
### Inference
```
python inference.py --jit_path MAHENet_model.pt --image_folder dataset/images --output_folder dataset/results
```
